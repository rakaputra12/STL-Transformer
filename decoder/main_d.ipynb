{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81b0a7e",
   "metadata": {},
   "source": [
    "### **Finetuning dense GPT-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eee6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import Conv1D\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "lm_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "block_size = 512\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False, truncation=False)\n",
    "\n",
    "tokenized = lm_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "def group_texts(examples):\n",
    "    all_ids = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "    total = (len(all_ids) // block_size) * block_size\n",
    "    chunks = [all_ids[i : i + block_size] for i in range(0, total, block_size)]\n",
    "    return {\n",
    "      \"input_ids\": chunks,\n",
    "      \"attention_mask\": [[1]*block_size]*len(chunks),\n",
    "      \"labels\": chunks.copy(),\n",
    "    }\n",
    "\n",
    "lm_splits = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized[\"train\"].column_names\n",
    ")\n",
    "\n",
    "train_ds, eval_ds = lm_splits[\"train\"].train_test_split(test_size=0.1, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76497427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "lm_collator  = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import math\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-dense-wikitext\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "\n",
    "    num_train_epochs=4,           \n",
    "    learning_rate= 5e-5, \n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200, \n",
    ")\n",
    "\n",
    "lm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds, \n",
    "    data_collator=lm_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "lm_trainer.train()\n",
    "lm_trainer.save_model()\n",
    "\n",
    "res = lm_trainer.evaluate()\n",
    "import math\n",
    "print(\"Perplexity:\", math.exp(res[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a4a69",
   "metadata": {},
   "source": [
    "### **Sparsification GPT-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import Conv1D\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "parameters_to_prune = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (torch.nn.Linear, Conv1D)):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "# Adjust amount to determine the sparsity level (e.g., 0.1 for 10%, 0.5 for 50%, etc.)\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.50,\n",
    ")\n",
    "\n",
    "for module, param_name in parameters_to_prune:\n",
    "    prune.remove(module, param_name)\n",
    "\n",
    "\n",
    "output_dir = \"gpt2-unstructured-50\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Pruned model saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f30580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Sparsification\n",
    "total_zeros, total_elems = 0, 0\n",
    "for module, _ in parameters_to_prune:\n",
    "    tensor = module.weight\n",
    "    total_zeros += int((tensor == 0).sum())\n",
    "    total_elems += tensor.numel()\n",
    "print(f\"Global sparsity: {total_zeros}/{total_elems} = {total_zeros/total_elems:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df503f71",
   "metadata": {},
   "source": [
    "### **Full Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b29c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-unstructured-50\")\n",
    "block_size = 512\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False, truncation=False)\n",
    "\n",
    "tokenized = lm_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "def group_texts(examples):\n",
    "    all_ids = list(chain.from_iterable(examples[\"input_ids\"]))\n",
    "    total = (len(all_ids) // block_size) * block_size\n",
    "    chunks = [all_ids[i : i + block_size] for i in range(0, total, block_size)]\n",
    "    return {\n",
    "      \"input_ids\": chunks,\n",
    "      \"attention_mask\": [[1]*block_size]*len(chunks),\n",
    "      \"labels\": chunks.copy(),\n",
    "    }\n",
    "\n",
    "lm_splits = tokenized.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    remove_columns=tokenized[\"train\"].column_names\n",
    ")\n",
    "\n",
    "train_ds, eval_ds = lm_splits[\"train\"].train_test_split(test_size=0.1, seed=42).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import math\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_output_gpt2_50_wikitext/train_eval\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "\n",
    "    num_train_epochs=4,           \n",
    "    learning_rate= 5e-5, \n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "    fp16=True \n",
    ")\n",
    "\n",
    "lm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds, \n",
    "    data_collator=lm_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "lm_trainer.train()\n",
    "lm_trainer.save_model()\n",
    "\n",
    "res = lm_trainer.evaluate()\n",
    "import math\n",
    "print(\"Perplexity:\", math.exp(res[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ffef0",
   "metadata": {},
   "source": [
    "### **Linear Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-unstructured-50\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") \n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9948f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from a Stack Overflow answer on freezing specific layers in PyTorch\n",
    "# Accessed on: 01.05.2025\n",
    "# Link: https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.lm_head.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Trainable params:\")\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"  \", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194112",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_output_gpt2_50_linear_wikitext/train_eval\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "\n",
    "    num_train_epochs=4,           \n",
    "    learning_rate= 5e-5, \n",
    "    weight_decay=0.00,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "    fp16=True \n",
    ")\n",
    "\n",
    "lm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds, \n",
    "    data_collator=lm_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "lm_trainer.train()\n",
    "lm_trainer.save_model()\n",
    "\n",
    "res = lm_trainer.evaluate()\n",
    "import math\n",
    "print(\"Perplexity:\", math.exp(res[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78944312",
   "metadata": {},
   "source": [
    "### **Full Finetuning mit KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"gpt2-dense-wikitext\").cuda()\n",
    "\n",
    "student = AutoModelForCausalLM.from_pretrained(\"gpt2-unstructured-50\").cuda()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0490de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code citation start [1]\n",
    "# Partially copied (with minor modifications) from:\n",
    "# Divesh R. Kubal, \"Knowledge Distillation Implementation End to End\"\n",
    "# GitHub, accessed on 03.05.2025\n",
    "# https://github.com/DiveshRKubal/transformers_model_production/blob/main/knowledge_distillation_implementation_end_to_end.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KDTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "class KDTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model.eval()\n",
    "        for p in self.teacher_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce        = outputs_student.loss\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "        logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        T     = self.args.temperature\n",
    "        kl = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            F.log_softmax(logits_student / T, dim=-1),\n",
    "            F.softmax(logits_teacher / T, dim=-1),\n",
    "        ) * (T * T)\n",
    "\n",
    "        alpha = self.args.alpha\n",
    "        loss = alpha * loss_ce + (1 - alpha) * kl\n",
    "\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "    \n",
    "# Code citation end [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "lm_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"./training_output_gpt2_50_mit_KD_wikitext\",\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "\n",
    "    learning_rate=1.5e-4,\n",
    "    fp16=True,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "\n",
    "    alpha=0.6,          \n",
    "    temperature=2.0\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "distil_trainer = KDTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=lm_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")\n",
    "\n",
    "distil_trainer.train()\n",
    "res = distil_trainer.evaluate()\n",
    "import math\n",
    "print(\"Distilled Perplexity KD:\", math.exp(res[\"eval_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c770a",
   "metadata": {},
   "source": [
    "### **Linear Finetuning mit KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"gpt2-dense-wikitext\").cuda()\n",
    "student = AutoModelForCausalLM.from_pretrained(\"gpt2-unstructured-50\").cuda()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33018c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in student.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in student.lm_head.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "lm_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"./Linear_KD\",\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "\n",
    "    learning_rate=1.5e-4,\n",
    "    fp16=True,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "\n",
    "    alpha=0.6,          \n",
    "    temperature=2.0\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "distil_trainer = KDTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=lm_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")\n",
    "\n",
    "distil_trainer.train()\n",
    "res = distil_trainer.evaluate()\n",
    "\n",
    "import math\n",
    "print(\"Distilled Perplexity KD:\", math.exp(res[\"eval_loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
