{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31c3a05",
   "metadata": {},
   "source": [
    "### **Finetuning dense BERT base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63766ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "tokenized = tokenized.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2333fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-base-dense-sst2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./bert-base-dense-sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2bbab",
   "metadata": {},
   "source": [
    "### **Sparsification Pretrained BERT base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef472dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import BertTokenizerFast, BertForPreTraining\n",
    "\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77270f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "to_prune = [\n",
    "    (m, \"weight\")\n",
    "    for _,m in model.named_modules()\n",
    "    if isinstance(m, torch.nn.Linear)\n",
    "]\n",
    "prune.global_unstructured(to_prune, prune.L1Unstructured, amount=0.8)\n",
    "\n",
    "for m, n in to_prune:\n",
    "    prune.remove(m, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142361fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Sparsification\n",
    "def count_zero_weights(model):\n",
    "    total, zero = 0, 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            total += param.numel()\n",
    "            zero += (param == 0).sum().item()\n",
    "    print(f\"Sparsity: {100 * zero / total:.2f}%\")\n",
    "\n",
    "count_zero_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the pruned model will be saved\n",
    "model.save_pretrained(\"bert_80\")\n",
    "tokenizer.save_pretrained(\"bert_80\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751ec51",
   "metadata": {},
   "source": [
    "### **Full Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"bert_80\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output_80_sst2\",\n",
    "    do_train=True, do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1.5e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8980e9b",
   "metadata": {},
   "source": [
    "### **Linear Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f28ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"bert_80\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from a Stack Overflow answer on freezing specific layers in PyTorch\n",
    "# Accessed on: 01.05.2025\n",
    "# Link: https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check the no freeze layer\n",
    "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output_80 LI_sst2\",\n",
    "    do_train=True, do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1.5e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158150d",
   "metadata": {},
   "source": [
    "### **Full Finetuning mit KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89493784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "\n",
    "student_checkpoint = \"bert_80\"\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(student_checkpoint)\n",
    "student = AutoModelForSequenceClassification.from_pretrained(student_checkpoint, config=student_config).cuda()\n",
    "\n",
    "\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\"bert-base-dense-sst2\").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code citation start [1]\n",
    "# Partially copied (with minor modifications) from:\n",
    "# Divesh R. Kubal, \"Knowledge Distillation Implementation End to End\"\n",
    "# GitHub, accessed on 03.05.2025\n",
    "# https://github.com/DiveshRKubal/transformers_model_production/blob/main/knowledge_distillation_implementation_end_to_end.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KDTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "class KDTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model.eval()\n",
    "        for p in self.teacher_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce        = outputs_student.loss\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "        logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        T     = self.args.temperature\n",
    "        kl = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            F.log_softmax(logits_student / T, dim=-1),\n",
    "            F.softmax(logits_teacher / T, dim=-1),\n",
    "        ) * (T * T)\n",
    "\n",
    "        alpha = self.args.alpha\n",
    "        loss = alpha * loss_ce + (1 - alpha) * kl\n",
    "\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "    \n",
    "# Code citation end [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"./KD\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    alpha=1.0,\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = KDTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher, \n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'], \n",
    "    eval_dataset=tokenized['validation'],\n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ba9bf",
   "metadata": {},
   "source": [
    "### **Linear Finetuning with KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc609e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "\n",
    "student_checkpoint = \"bert_80\"\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(student_checkpoint)\n",
    "student = AutoModelForSequenceClassification.from_pretrained(student_checkpoint, config=student_config).cuda()\n",
    "\n",
    "\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\"bert-base-dense-sst2\").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from a Stack Overflow answer on freezing specific layers in PyTorch\n",
    "# Accessed on: 01.05.2025\n",
    "# Link: https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "\n",
    "for name, param in student.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check the no freeze layer\n",
    "trainable = [n for n, p in student.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"./KD\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    alpha=1.0,\n",
    "    temperature=2.0\n",
    ")\n",
    "\n",
    "\n",
    "trainer = KDTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher, \n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'], \n",
    "    eval_dataset=tokenized['validation'],\n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
