{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c31eaa",
   "metadata": {},
   "source": [
    "### **Finetuning dense T5-Small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code citation start [1]\n",
    "# Copied from Hugging Face Transformers documentation (Summarization)\n",
    "# Author: Hugging Face\n",
    "# Title: Summarization \n",
    "# Accessed on: 30.04.2025\n",
    "# Link: https://huggingface.co/docs/transformers/en/tasks/summarization\n",
    "# License: Apache 2.0\n",
    "# License link: https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f3f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\STL-Transformer\\virt_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b30751b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c497f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e596b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 248/248 [00:00<00:00, 637.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65da6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707fec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d5a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd0b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46456cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 13:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.719880</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.505391</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.441553</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.423877</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"finetuned_t5_small\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e913c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code citation end [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb388bc",
   "metadata": {},
   "source": [
    "### **Sparsification Pretrained Model T5-Small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be129a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn.utils.prune as prune_utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90ee2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_prune = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        modules_to_prune.append((module, 'weight'))\n",
    "\n",
    "# Adjust amount to determine the sparsity level (e.g., 0.1 for 10%, 0.5 for 50%, etc.)\n",
    "prune_utils.global_unstructured(\n",
    "    modules_to_prune,\n",
    "    pruning_method=prune_utils.L1Unstructured,\n",
    "    amount=0.3  \n",
    ")\n",
    "\n",
    "for module, _ in modules_to_prune:\n",
    "    prune.remove(module, 'weight')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b91acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 29.99%\n"
     ]
    }
   ],
   "source": [
    "# Check the Sparsification\n",
    "def count_zero_weights(model):\n",
    "    total, zero = 0, 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            total += param.numel()\n",
    "            zero += (param == 0).sum().item()\n",
    "    print(f\"Sparsity: {100 * zero / total:.2f}%\")\n",
    "\n",
    "count_zero_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d84541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the pruned model will be saved\n",
    "model.save_pretrained(\"t5-small_30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0622f6",
   "metadata": {},
   "source": [
    "### **Full Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e62d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint =\"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6ac59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model =\"t5-small_30\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"FT_t5_small_30\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d30a4f",
   "metadata": {},
   "source": [
    "### **Linear Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "df181f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small_30\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d639bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from a Stack Overflow answer on freezing specific layers in PyTorch\n",
    "# Accessed on: 01.05.2025\n",
    "# Link: https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.lm_head.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e85d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "  shared.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable parameters:\")\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\" \", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c237704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ef0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"LF_t5_small_30\",   \n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset= tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619ada3",
   "metadata": {},
   "source": [
    "### **Full Finetuning mit KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "76b15b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "teacher = T5ForConditionalGeneration.from_pretrained(\"finetuned_t5_small\").eval().cuda()\n",
    "\n",
    "student = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small_30\").train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "374028ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 128\n",
    "\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_data = load_dataset(\"billsum\", split=\"train[:2%]\").map(preprocess_function, batched=True)\n",
    "val_data = load_dataset(\"billsum\", split=\"test[:2%]\").map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5bb83a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    \n",
    "    final_result = {}\n",
    "    for k, v in result.items():\n",
    "        if hasattr(v, \"mid\"):\n",
    "            final_result[k] = round(v.mid.fmeasure, 4)\n",
    "        else:\n",
    "            final_result[k] = round(v, 4)\n",
    "\n",
    "   \n",
    "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
    "    final_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "066fc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code citation start [1]\n",
    "# Partially copied (with minor modifications) from:\n",
    "# Divesh R. Kubal, \"Knowledge Distillation Implementation End to End\"\n",
    "# GitHub, accessed on 03.05.2025\n",
    "# https://github.com/DiveshRKubal/transformers_model_production/blob/main/knowledge_distillation_implementation_end_to_end.ipynb\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class KDTrainingArguments(Seq2SeqTrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "class KDSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model.eval()\n",
    "        for p in self.teacher_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # note **kwargs here\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # 1) student forward\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce        = outputs_student.loss\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        # 2) teacher forward\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "        logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        # 3) distillation loss\n",
    "        T     = self.args.temperature\n",
    "        kl = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            F.log_softmax(logits_student / T, dim=-1),\n",
    "            F.softmax(logits_teacher / T, dim=-1),\n",
    "        ) * (T * T)\n",
    "\n",
    "        # 4) combined loss\n",
    "        alpha = self.args.alpha\n",
    "        loss = alpha * loss_ce + (1 - alpha) * kl\n",
    "\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "    \n",
    "# Code citation end [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"FT-KD-T5-Small\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    alpha=0.3,       \n",
    "    temperature=4.0,   \n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = KDSeq2SeqTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d14967",
   "metadata": {},
   "source": [
    "### **Linear Finetuning mit KD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "48f63d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "teacher = T5ForConditionalGeneration.from_pretrained(\"finetuned_t5_small\").eval().cuda()\n",
    "\n",
    "student = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small_30\").train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6722d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in student.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in student.lm_head.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "379485f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "  shared.weight\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable parameters:\")\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\" \", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ddfcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 128\n",
    "\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_data = load_dataset(\"billsum\", split=\"train[:2%]\").map(preprocess_function, batched=True)\n",
    "val_data = load_dataset(\"billsum\", split=\"test[:2%]\").map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f844a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itba000035\\AppData\\Local\\Temp\\ipykernel_16864\\3882849167.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KDSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.261447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.260193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.257794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.257625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=447.8892415364583, metrics={'train_runtime': 40.2349, 'train_samples_per_second': 37.679, 'train_steps_per_second': 2.386, 'total_flos': 205178171031552.0, 'train_loss': 447.8892415364583, 'epoch': 4.0})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = KDTrainingArguments(\n",
    "    output_dir=\"LT-KD-T5-Small\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    alpha=0.3,       \n",
    "    temperature=4.0,   \n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = KDSeq2SeqTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
